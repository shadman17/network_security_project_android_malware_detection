{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952155a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350998ce-f832-4094-bfbc-e3e55a8ce075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8abe7b7-a481-46ee-99d7-f3b1bef93d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10171 images belonging to 5 classes.\n",
      "Found 4363 images belonging to 5 classes.\n",
      "Model: \"MalwareClassifier_CNN_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 180, 180, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 178, 178, 10)      280       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 176, 176, 10)      910       \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)     (None, 88, 88, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 86, 86, 16)        1456      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, 84, 84, 16)        2320      \n",
      "_________________________________________________________________\n",
      "maxpool_2 (MaxPooling2D)     (None, 42, 42, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_layer (Flatten)      (None, 28224)             0         \n",
      "_________________________________________________________________\n",
      "FC_1 (Dense)                 (None, 50)                1411250   \n",
      "_________________________________________________________________\n",
      "leaky_ReLu_1 (LeakyReLU)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "FC_2 (Dense)                 (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "leaky_ReLu_2 (LeakyReLU)     (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 1,419,021\n",
      "Trainable params: 1,419,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      " 22/318 [=>............................] - ETA: 3:01 - loss: 1.4671 - acc: 0.4645"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "from utils import gen_maker, CustomCallback\n",
    "from networks import model_maker\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import tensorflow_addons as tfa\n",
    "import pickle as pkl\n",
    "from keras.callbacks import CSVLogger\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 1, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a4e19-704c-43f2-884e-e3c6d5567530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pickle\n",
    "path = 'D:\\\\Buet\\\\Semester_3\\\\Network_Security\\\\New folder (2)\\\\trained_models\\\\'\n",
    "dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "\n",
    "print(dstpath)\n",
    "\n",
    "with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), \"rb\") as f:\n",
    "    content = pickle.load(f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b7a9e-4a1d-4091-80b7-be68caa927e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 2, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dacf8cb-2a78-4092-920b-cc9c955cfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 3, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc12266-6a3e-48dc-80af-d3f5b2d602dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 4, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12ecc5-b2b3-4e38-aeaa-03126ba84639",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 5, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc9c7f-c3cf-493d-873c-89fa44914394",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 6, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c621eeb-79c7-47a6-9f7f-db53bd5c5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 7, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606d3ac-5358-4aa2-b16c-063b53424b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', type=int, default = 15, help='number of total epochs')\n",
    "#parser.add_argument('--init_epoch', type=int, default = 0, help='initial epoch')\n",
    "parser.add_argument('--train_dir', type=str, default = '../dataset/train/', help = 'training data folder path')\n",
    "parser.add_argument('--val_dir', type=str, default = '../dataset/val/', help = 'validation data folder path')\n",
    "parser.add_argument('--model_id', type = int, default = 8, help = 'model ID. it can bee 1, 2, 3, 4, 5, 6, 7 or 8')\n",
    "parser.add_argument('--load_model', type = int, default = 0, help = 'if 1, you should specify the address of the previously trained model to load it')\n",
    "parser.add_argument('--load_path', type = str, default= None, help = 'path to pre-trained models')\n",
    "parser.add_argument('--backup_path', type = str, default = '../trained_models/', help ='path to store the model')\n",
    "#parser.add_argument('--bach_size', type = int, default = 32, help = 'batch size for training')\n",
    "parser.add_argument('--mode', type = str, default = 'categorical', help = 'classification mode')\n",
    "parser.add_argument('--target_size', type = int, default = 180, help ='size of the input images')\n",
    "parser.add_argument('--batch_size', type = int, default = 32, help ='batch size')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "    \n",
    "def main():\n",
    "\n",
    "    path = '../trained_models/'\n",
    "\n",
    "    dstpath = os.path.join(path, f\"id-{args.model_id}_target-size-{args.target_size}\")\n",
    "    os.makedirs(dstpath, exist_ok=True)\n",
    "\n",
    "    train_gen, val_gen = gen_maker(args.train_dir,\n",
    "     args.val_dir,\n",
    "      target_size=(args.target_size, args.target_size),\n",
    "       batch_size=args.batch_size,\n",
    "       mode=args.mode)\n",
    "\n",
    "    csv_log = CSVLogger(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.csv'), separator=',', append=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    clbk = CustomCallback(val_gen, args.backup_path, args.model_id)\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.000001\n",
    "    model = model_maker((args.target_size, args.target_size), args.model_id)\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='acc')\n",
    "        \n",
    "    if args.load_model:\n",
    "        model = tf.keras.load_model(args.load_path)\n",
    "    results = model.fit(train_gen,\n",
    "                    epochs = args.epochs,\n",
    "                    validation_data = (val_gen),\n",
    "                    callbacks = [clbk, csv_log], initial_epoch=0)\n",
    "\n",
    "    y_pred_valid = model.predict(val_gen)\n",
    "\n",
    "    history = {'train loss' : results.history['loss'],\n",
    "           'val loss' : results.history['val_loss'], \n",
    "           'train acc': results.history['acc'],\n",
    "           'val acc' : results.history['val_acc'],\n",
    "           'y_true_valid': val_gen.classes,\n",
    "           'y_pred_valid': y_pred_valid, \n",
    "           'id': args.model_id}\n",
    "\n",
    "\n",
    "    \n",
    "    with open(os.path.join(dstpath, f'id-{args.model_id}_target-size-{args.target_size}.pkl'), 'wb') as f:\n",
    "        pkl.dump(history, f)\n",
    "    \n",
    "        \n",
    "    # with open(args.backup_path + path2 +'id-{}-{}.pkl'.format(args.model_id, args.target_size), 'wb') as f:\n",
    "    #     pkl.dump(history, f)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(results.history['loss'], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot(results.history['val_loss'], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train loss', 'val loss'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot([0, *results.history['acc']], '-', color = [0, 0, 1, 1])\n",
    "    plt.plot([0, *results.history['val_acc']], '-', color = [1, 0, 0, 1])\n",
    "    plt.legend(['train acc', 'val acc'])\n",
    "    plt.savefig(os.path.join(dstpath, 'charts.png'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
